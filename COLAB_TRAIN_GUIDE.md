# HÆ°á»›ng dáº«n Train ACE-Step LoRA trÃªn Google Colab

## ğŸ“‹ YÃªu cáº§u

- Google Colab Pro/Pro+ (Ä‘á»ƒ cÃ³ GPU tá»‘t hÆ¡n vÃ  runtime lÃ¢u hÆ¡n)
- Google Drive (Ä‘á»ƒ lÆ°u checkpoint vÃ  dataset)
- Dataset Ä‘Ã£ Ä‘Æ°á»£c convert sang HuggingFace format

## ğŸš€ BÆ°á»›c 1: Chuáº©n bá»‹ Dataset trÃªn Google Drive

1. Upload dataset lÃªn Google Drive:
   - Folder `vi_lora_dataset` (Ä‘Ã£ convert)
   - Hoáº·c upload folder `data` vÃ  convert trÃªn Colab

2. Upload config file:
   - `config/vi_lora_config.json`

## ğŸš€ BÆ°á»›c 2: Táº¡o Notebook Colab má»›i

1. Má»Ÿ [Google Colab](https://colab.research.google.com/)
2. Táº¡o notebook má»›i
3. Chá»n Runtime â†’ Change runtime type â†’ GPU (T4/V100/A100)

## ğŸš€ BÆ°á»›c 3: Setup mÃ´i trÆ°á»ng

Cháº¡y cÃ¡c cell sau trong notebook:

### Cell 1: Mount Google Drive
```python
from google.colab import drive
drive.mount('/content/drive')
```

### Cell 2: Clone repository
```python
!git clone https://github.com/ace-step/ACE-Step.git
%cd ACE-Step
```

### Cell 3: CÃ i Ä‘áº·t dependencies
```python
!pip install -r requirements.txt
!pip install pytorch-lightning
!pip install transformers accelerate
```

### Cell 4: Setup dataset (náº¿u chÆ°a convert)
```python
# Náº¿u dataset chÆ°a Ä‘Æ°á»£c convert, cháº¡y:
# !python convert2hf_dataset.py --data_dir /content/drive/MyDrive/data --repeat_count 2000 --output_name vi_lora_dataset
```

## ğŸš€ BÆ°á»›c 4: Train LoRA

### Cell 5: Cháº¡y training
```python
import os

# Táº¡o thÆ° má»¥c output
os.makedirs("/content/drive/MyDrive/ace_step_outputs/checkpoints", exist_ok=True)
os.makedirs("/content/drive/MyDrive/ace_step_outputs/logs", exist_ok=True)

# Lá»‡nh train
!python trainer.py \
    --num_nodes 1 \
    --devices 1 \
    --dataset_path "/content/drive/MyDrive/vi_lora_dataset" \
    --exp_name "vi_lora_small" \
    --lora_config_path "config/vi_lora_config.json" \
    --learning_rate 1e-4 \
    --accumulate_grad_batches 4 \
    --precision 16 \
    --num_workers 2 \
    --max_steps 20000 \
    --every_n_train_steps 500 \
    --shift 3.0 \
    --checkpoint_dir "/content/drive/MyDrive/ace_step_outputs/checkpoints" \
    --logger_dir "/content/drive/MyDrive/ace_step_outputs/logs" \
    --epochs -1 \
    --every_plot_step 2000 \
    --gradient_clip_val 0.5 \
    --gradient_clip_algorithm "norm"
```

## âš ï¸ LÆ°u Ã½ quan trá»ng

### 1. Runtime timeout
- Colab free: ~12 giá» timeout
- Colab Pro: ~24 giá» timeout
- **Giáº£i phÃ¡p**: LÆ°u checkpoint thÆ°á»ng xuyÃªn (má»—i 500 steps) vÃ  resume sau

### 2. Resume tá»« checkpoint
```python
# TÃ¬m checkpoint má»›i nháº¥t
import glob
checkpoints = glob.glob("/content/drive/MyDrive/ace_step_outputs/logs/vi_lora/lightning_logs/*/checkpoints/*.ckpt")
latest_checkpoint = max(checkpoints, key=os.path.getctime) if checkpoints else None

# ThÃªm --ckpt_path náº¿u cÃ³ checkpoint
ckpt_arg = f"--ckpt_path {latest_checkpoint}" if latest_checkpoint else ""

!python trainer.py \
    ... (cÃ¡c tham sá»‘ khÃ¡c) ... \
    {ckpt_arg}
```

### 3. Tá»‘i Æ°u cho Colab GPU
- **T4 (16GB)**: DÃ¹ng `--accumulate_grad_batches 4`, `--precision 16`
- **V100 (16GB)**: CÃ³ thá»ƒ tÄƒng `--accumulate_grad_batches 8`
- **A100 (40GB)**: CÃ³ thá»ƒ tÄƒng batch size vÃ  giáº£m `accumulate_grad_batches`

### 4. LÆ°u checkpoint lÃªn Drive
- Checkpoint tá»± Ä‘á»™ng lÆ°u vÃ o `--checkpoint_dir` (Ä‘Ã£ set lÃ  Google Drive)
- NÃªn backup checkpoint quan trá»ng vÃ o folder riÃªng

### 5. Monitor training
```python
# Xem log trong Colab
!tail -f /content/drive/MyDrive/ace_step_outputs/logs/vi_lora/lightning_logs/*/events.out.tfevents.*
```

## ğŸ“Š So sÃ¡nh Colab vs Local

| TiÃªu chÃ­ | Colab | Local (RTX 3050) |
|----------|-------|-----------------|
| GPU | T4/V100/A100 | RTX 3050 (6GB) |
| Tá»‘c Ä‘á»™ | Nhanh hÆ¡n (T4 ~= RTX 3050) | Cháº­m hÆ¡n |
| Thá»i gian | Giá»›i háº¡n 12-24h | KhÃ´ng giá»›i háº¡n |
| Chi phÃ­ | Free/Pro ($10/thÃ¡ng) | Äiá»‡n + hao mÃ²n |
| á»”n Ä‘á»‹nh | CÃ³ thá»ƒ bá»‹ disconnect | á»”n Ä‘á»‹nh hÆ¡n |
| Checkpoint | Cáº§n lÆ°u lÃªn Drive | LÆ°u local |

## ğŸ¯ Khuyáº¿n nghá»‹

1. **Train ban Ä‘áº§u trÃªn Colab**: Äá»ƒ test vÃ  xem tá»‘c Ä‘á»™
2. **Train lÃ¢u dÃ i trÃªn Local**: Náº¿u cÃ³ thá»i gian vÃ  muá»‘n á»•n Ä‘á»‹nh
3. **Hybrid**: Train trÃªn Colab ban Ä‘áº§u, sau Ä‘Ã³ download checkpoint vá» local Ä‘á»ƒ tiáº¿p tá»¥c

## ğŸ”§ Troubleshooting

### Lá»—i: Out of Memory
- Giáº£m `--accumulate_grad_batches` xuá»‘ng 2 hoáº·c 1
- Giáº£m `--num_workers` xuá»‘ng 0

### Lá»—i: Runtime disconnected
- Resume tá»« checkpoint má»›i nháº¥t
- TÄƒng táº§n suáº¥t lÆ°u checkpoint (`--every_n_train_steps 200`)

### Lá»—i: Drive quota full
- XÃ³a checkpoint cÅ©
- Chá»‰ giá»¯ checkpoint má»›i nháº¥t vÃ  cÃ¡c checkpoint quan trá»ng

