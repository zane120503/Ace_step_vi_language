# HÆ°á»›ng dáº«n Train ACE-Step LoRA trÃªn Google Colab - Tá»«ng BÆ°á»›c Chi Tiáº¿t

## ğŸ“‹ Chuáº©n bá»‹ trÆ°á»›c khi báº¯t Ä‘áº§u

### 1. Cáº§n cÃ³:
- âœ… Google Account
- âœ… Google Drive (Ä‘á»ƒ lÆ°u dataset vÃ  checkpoint)
- âœ… Dataset Ä‘Ã£ Ä‘Æ°á»£c convert sang HuggingFace format (`vi_lora_dataset`)
- âœ… File config: `config/vi_lora_config.json`

### 2. Upload lÃªn Google Drive:
- Táº¡o folder `MyDrive/ace_step_data/`
- Upload folder `vi_lora_dataset` vÃ o Ä‘Ã³
- Upload file `config/vi_lora_config.json` vÃ o Ä‘Ã³

---

## ğŸš€ BÆ¯á»šC 1: Má»Ÿ Google Colab

1. Truy cáº­p: https://colab.research.google.com/
2. ÄÄƒng nháº­p báº±ng Google Account
3. Click **"New notebook"** Ä‘á»ƒ táº¡o notebook má»›i
4. Äáº·t tÃªn notebook: `ACE-Step LoRA Training`

---

## ğŸš€ BÆ¯á»šC 2: Chá»n GPU

1. Click **Runtime** â†’ **Change runtime type**
2. Chá»n:
   - **Hardware accelerator**: `GPU`
   - **GPU type**: 
     - **Free**: T4 (tá»± Ä‘á»™ng)
     - **Pro**: T4/V100 (tÃ¹y may máº¯n)
     - **Pro+**: A100 (tÃ¹y may máº¯n)
3. Click **Save**

---

## ğŸš€ BÆ¯á»šC 3: Mount Google Drive

**Táº¡o cell má»›i vÃ  cháº¡y:**

```python
from google.colab import drive
drive.mount('/content/drive')
```

**Káº¿t quáº£:**
- Sáº½ hiá»‡n link Ä‘á»ƒ authorize
- Click link â†’ chá»n Google Account â†’ Copy mÃ£
- Paste mÃ£ vÃ o Ã´ input â†’ Enter
- Sáº½ tháº¥y: `Mounted at /content/drive`

---

## ğŸš€ BÆ¯á»šC 4: Clone Repository

**Táº¡o cell má»›i vÃ  cháº¡y:**

```python
# Clone tá»« repository tiáº¿ng Viá»‡t (Ä‘Ã£ cÃ³ config vÃ  dataset cho tiáº¿ng Viá»‡t)
!git clone https://github.com/zane120503/Ace_step_vi_language.git
%cd Ace_step_vi_language
```

**Káº¿t quáº£:**
- Repository Ä‘Æ°á»£c clone vÃ o `/content/ACE-Step`
- ÄÃ£ chuyá»ƒn vÃ o thÆ° má»¥c ACE-Step

---

## ğŸš€ BÆ¯á»šC 5: CÃ i Ä‘áº·t Dependencies

**Táº¡o cell má»›i vÃ  cháº¡y:**

```python
print("ğŸ”§ Äang fix cÃ¡c dependency conflicts...")

# BÆ°á»›c 1: Uninstall cÃ¡c packages cÃ³ conflict Ä‘á»ƒ clean install
!pip uninstall -y numpy protobuf fsspec tensorboard 2>/dev/null || true

# BÆ°á»›c 2: CÃ i Ä‘áº·t cÃ¡c packages vá»›i version cá»‘ Ä‘á»‹nh (--no-deps Ä‘á»ƒ trÃ¡nh conflicts)
!pip install -q --no-deps "numpy>=1.26.0,<2.1.0"
!pip install -q --no-deps "protobuf>=3.20.3,<6.0.0,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5"
!pip install -q --no-deps "fsspec>=2023.1.0,<=2024.12.0"
!pip install -q --no-deps "tensorboard==2.19.0"
!pip install -q "jedi>=0.16"

# BÆ°á»›c 3: CÃ i Ä‘áº·t dependencies chÃ­nh
print("ğŸ“¦ Äang cÃ i Ä‘áº·t dependencies chÃ­nh...")
!pip install -q pytorch-lightning transformers accelerate

# BÆ°á»›c 4: CÃ i Ä‘áº·t requirements.txt vá»›i --no-deps Ä‘á»ƒ trÃ¡nh conflicts
print("ğŸ“¦ Äang cÃ i Ä‘áº·t requirements.txt (bá» qua dependency checks)...")
!pip install -q --no-deps -r requirements.txt

# BÆ°á»›c 5: Force reinstall cÃ¡c packages quan trá»ng vá»›i version Ä‘Ãºng
print("ğŸ”§ Äang lock cÃ¡c packages quan trá»ng á»Ÿ version Ä‘Ãºng...")
!pip install -q --force-reinstall --no-deps "numpy>=1.26.0,<2.1.0" 2>/dev/null || true
!pip install -q --force-reinstall --no-deps "protobuf>=3.20.3,<6.0.0,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5" 2>/dev/null || true
!pip install -q --force-reinstall --no-deps "fsspec>=2023.1.0,<=2024.12.0" 2>/dev/null || true
!pip install -q --force-reinstall --no-deps "tensorboard==2.19.0" 2>/dev/null || true

print("âœ“ ÄÃ£ cÃ i Ä‘áº·t dependencies")
print("â„¹ ÄÃ£ fix vÃ  lock cÃ¡c conflicts: numpy, protobuf, tensorboard, fsspec, jedi")
print("âš ï¸  Má»™t sá»‘ warnings vá» gcsfs/fsspec cÃ³ thá»ƒ xuáº¥t hiá»‡n nhÆ°ng KHÃ”NG áº£nh hÆ°á»Ÿng training")
print("âš ï¸  CÃ¡c warnings vá» dependency conflicts cÃ³ thá»ƒ bá» qua náº¿u training váº«n cháº¡y Ä‘Æ°á»£c")
```

**LÆ°u Ã½:**
- CÃ³ thá»ƒ máº¥t 5-10 phÃºt
- Script Ä‘Ã£ tá»± Ä‘á»™ng xá»­ lÃ½ cÃ¡c dependency conflicts phá»• biáº¿n
- CÃ³ thá»ƒ váº«n cÃ³ má»™t sá»‘ warnings, nhÆ°ng **khÃ´ng áº£nh hÆ°á»Ÿng Ä‘áº¿n training**
- Náº¿u cÃ³ lá»—i nghiÃªm trá»ng khÃ¡c, thá»­ cháº¡y láº¡i cell

---

## ğŸš€ BÆ¯á»šC 6: Kiá»ƒm tra Dataset vÃ  Config

**Táº¡o cell má»›i vÃ  cháº¡y:**

```python
import os
import glob

# BÆ°á»›c 1: Kiá»ƒm tra Google Drive Ä‘Ã£ Ä‘Æ°á»£c mount chÆ°a
if not os.path.exists("/content/drive"):
    print("âŒ Google Drive chÆ°a Ä‘Æ°á»£c mount!")
    print("   Vui lÃ²ng cháº¡y BÆ¯á»šC 3 (Mount Google Drive) trÆ°á»›c!")
else:
    print("âœ“ Google Drive Ä‘Ã£ Ä‘Æ°á»£c mount")
    
    # BÆ°á»›c 2: TÃ¬m táº¥t cáº£ cÃ¡c folder cÃ³ tÃªn "vi_lora_dataset" trÃªn Drive
    print("ğŸ” Äang tÃ¬m dataset trÃªn Google Drive...")
    drive_root = "/content/drive/MyDrive"
    
    found_datasets = []
    for root, dirs, files in os.walk(drive_root):
        if "vi_lora_dataset" in dirs:
            full_path = os.path.join(root, "vi_lora_dataset")
            if os.path.isdir(full_path):
                file_count = len(os.listdir(full_path))
                found_datasets.append((full_path, file_count))
                print(f"âœ“ TÃ¬m tháº¥y: {full_path} ({file_count} files)")
    
    # BÆ°á»›c 3: Chá»n dataset phÃ¹ há»£p
    dataset_path = None
    if found_datasets:
        # Æ¯u tiÃªn dataset trong ace_step_data
        for path, count in found_datasets:
            if "ace_step_data" in path:
                dataset_path = path
                print(f"\nâœ“ Dataset tÃ¬m tháº¥y táº¡i: {path}")
                print(f"âœ“ Sá»‘ file trong dataset: {count}")
                break
        
        # Náº¿u khÃ´ng cÃ³ trong ace_step_data, dÃ¹ng dataset Ä‘áº§u tiÃªn
        if not dataset_path:
            dataset_path, count = found_datasets[0]
            print(f"\nâœ“ Dataset tÃ¬m tháº¥y táº¡i: {dataset_path}")
            print(f"âœ“ Sá»‘ file trong dataset: {count}")
            if len(found_datasets) > 1:
                print(f"âš ï¸  TÃ¬m tháº¥y {len(found_datasets)} dataset, Ä‘ang dÃ¹ng: {dataset_path}")
    else:
        print("\nâŒ KhÃ´ng tÃ¬m tháº¥y dataset!")
        print("   Vui lÃ²ng upload dataset lÃªn Google Drive")
        print("   CÃ³ thá»ƒ Ä‘áº·t á»Ÿ báº¥t ká»³ Ä‘Ã¢u trong MyDrive")

# BÆ°á»›c 4: Kiá»ƒm tra vÃ  copy config
config_paths = [
    "/content/drive/MyDrive/MyDrive/ace_step_data/config/vi_lora_config.json",  # TrÆ°á»ng há»£p cÃ³ MyDrive trong MyDrive
    "/content/drive/MyDrive/ace_step_data/config/vi_lora_config.json",
    "/content/drive/MyDrive/config/vi_lora_config.json",
    "/content/drive/MyDrive/vi_lora_config.json",
]

config_found = False
for config_path in config_paths:
    if os.path.exists(config_path):
        !cp "{config_path}" config/vi_lora_config.json
        print(f"âœ“ Config file Ä‘Ã£ Ä‘Æ°á»£c copy tá»«: {config_path}")
        config_found = True
        break

if not config_found:
    print("âš ï¸  KhÃ´ng tÃ¬m tháº¥y config file!")
    print("   CÃ³ thá»ƒ sá»­ dá»¥ng config máº·c Ä‘á»‹nh trong repo")
    if os.path.exists("config/vi_lora_config.json"):
        print("âœ“ ÄÃ£ tÃ¬m tháº¥y config trong repo")
    else:
        print("âŒ Cáº§n táº¡o hoáº·c upload config file")
```

**Káº¿t quáº£ mong Ä‘á»£i:**
- âœ“ Google Drive Ä‘Ã£ Ä‘Æ°á»£c mount
- âœ“ Dataset tÃ¬m tháº¥y táº¡i: `/content/drive/MyDrive/.../vi_lora_dataset`
- âœ“ Sá»‘ file trong dataset: ...
- âœ“ Config file Ä‘Ã£ Ä‘Æ°á»£c copy (náº¿u cÃ³)

---

## ğŸš€ BÆ¯á»šC 7: Táº¡o ThÆ° Má»¥c Output

**Táº¡o cell má»›i vÃ  cháº¡y:**

```python
import os

# Táº¡o thÆ° má»¥c trÃªn Google Drive Ä‘á»ƒ lÆ°u checkpoint vÃ  log
checkpoint_dir = "/content/drive/MyDrive/ace_step_outputs/checkpoints"
log_dir = "/content/drive/MyDrive/ace_step_outputs/logs"

os.makedirs(checkpoint_dir, exist_ok=True)
os.makedirs(log_dir, exist_ok=True)

print(f"âœ“ ÄÃ£ táº¡o thÆ° má»¥c checkpoint: {checkpoint_dir}")
print(f"âœ“ ÄÃ£ táº¡o thÆ° má»¥c log: {log_dir}")
```

**Káº¿t quáº£ mong Ä‘á»£i:**
- âœ“ ÄÃ£ táº¡o thÆ° má»¥c checkpoint: ...
- âœ“ ÄÃ£ táº¡o thÆ° má»¥c log: ...

---

## ğŸš€ BÆ¯á»šC 8: Kiá»ƒm tra GPU

**Táº¡o cell má»›i vÃ  cháº¡y:**

```python
import torch

print("ğŸ” Kiá»ƒm tra GPU...")
if torch.cuda.is_available():
    gpu_name = torch.cuda.get_device_name(0)
    gpu_memory = torch.cuda.get_device_properties(0).total_memory / (1024**3)
    print(f"âœ“ GPU tÃ¬m tháº¥y: {gpu_name}")
    print(f"âœ“ VRAM: {gpu_memory:.2f} GB")
    print(f"âœ“ CUDA available: {torch.cuda.is_available()}")
    print(f"âœ“ CUDA version: {torch.version.cuda}")
    gpu_ok = True
else:
    print("âŒ GPU khÃ´ng available!")
    print("   Vui lÃ²ng chá»n Runtime â†’ Change runtime type â†’ GPU")
    print("   Sau Ä‘Ã³ restart runtime vÃ  cháº¡y láº¡i cell nÃ y")
    gpu_ok = False
```

**Káº¿t quáº£ mong Ä‘á»£i:**
- âœ“ GPU tÃ¬m tháº¥y: Tesla T4 (hoáº·c V100/A100)
- âœ“ VRAM: 16.00 GB (hoáº·c tÆ°Æ¡ng á»©ng)
- âœ“ CUDA available: True

---

## ğŸš€ BÆ¯á»šC 9: TÃ¬m Checkpoint (Náº¿u Resume)

**Táº¡o cell má»›i vÃ  cháº¡y:**

```python
import glob
import os

# TÃ¬m checkpoint má»›i nháº¥t (náº¿u cÃ³)
log_dir = "/content/drive/MyDrive/ace_step_outputs/logs/vi_lora/lightning_logs"
checkpoints = glob.glob(f"{log_dir}/*/checkpoints/*.ckpt") if os.path.exists(log_dir) else []

if checkpoints:
    latest_checkpoint = max(checkpoints, key=os.path.getctime)
    print(f"âœ“ TÃ¬m tháº¥y checkpoint má»›i nháº¥t: {latest_checkpoint}")
    print(f"  Sáº½ resume tá»« checkpoint nÃ y")
    ckpt_path = latest_checkpoint
else:
    print("â„¹ ChÆ°a cÃ³ checkpoint, sáº½ train tá»« Ä‘áº§u")
    ckpt_path = None
```

**Káº¿t quáº£:**
- Náº¿u cÃ³ checkpoint: âœ“ TÃ¬m tháº¥y checkpoint má»›i nháº¥t: ...
- Náº¿u chÆ°a cÃ³: â„¹ ChÆ°a cÃ³ checkpoint, sáº½ train tá»« Ä‘áº§u

---

## ğŸš€ BÆ¯á»šC 10: Báº¯t Äáº§u Training

**Táº¡o cell má»›i vÃ  cháº¡y:**

```python
import os

# Kiá»ƒm tra dataset_path Ä‘Ã£ Ä‘Æ°á»£c tÃ¬m tháº¥y á»Ÿ BÆ°á»›c 6 chÆ°a
if 'dataset_path' not in locals() or dataset_path is None:
    print("ğŸ” Äang tÃ¬m láº¡i dataset...")
    # TÃ¬m láº¡i dataset
    drive_root = "/content/drive/MyDrive"
    found_datasets = []
    
    for root, dirs, files in os.walk(drive_root):
        if "vi_lora_dataset" in dirs:
            full_path = os.path.join(root, "vi_lora_dataset")
            if os.path.isdir(full_path):
                found_datasets.append(full_path)
    
    if found_datasets:
        # Æ¯u tiÃªn dataset trong ace_step_data
        for path in found_datasets:
            if "ace_step_data" in path:
                dataset_path = path
                break
        if not dataset_path:
            dataset_path = found_datasets[0]
        print(f"âœ“ TÃ¬m tháº¥y dataset: {dataset_path}")
    else:
        print("âŒ Váº«n khÃ´ng tÃ¬m tháº¥y dataset!")
        raise FileNotFoundError("Dataset not found!")

# XÃ¡c nháº­n dataset path
print(f"ğŸ“‚ Sá»­ dá»¥ng dataset: {dataset_path}")
if not os.path.exists(dataset_path):
    raise FileNotFoundError(f"Dataset path khÃ´ng tá»“n táº¡i: {dataset_path}")

# Tham sá»‘ training
checkpoint_dir = "/content/drive/MyDrive/ace_step_outputs/checkpoints"
log_dir = "/content/drive/MyDrive/ace_step_outputs/logs"

# Kiá»ƒm tra GPU trÆ°á»›c khi train
if 'gpu_ok' not in locals() or not gpu_ok:
    print("âš ï¸  GPU chÆ°a Ä‘Æ°á»£c kiá»ƒm tra!")
    print("   Vui lÃ²ng cháº¡y BÆ¯á»šC 8 (Kiá»ƒm tra GPU) trÆ°á»›c!")
    raise RuntimeError("GPU not checked!")

# Build command
cmd = f"""python trainer.py \\
    --num_nodes 1 \\
    --devices 1 \\
    --dataset_path "{dataset_path}" \\
    --exp_name "vi_lora_small" \\
    --lora_config_path "config/vi_lora_config.json" \\
    --learning_rate 1e-4 \\
    --accumulate_grad_batches 4 \\
    --precision 16 \\
    --num_workers 2 \\
    --max_steps 20000 \\
    --every_n_train_steps 100 \\
    --shift 3.0 \\
    --checkpoint_dir "{checkpoint_dir}" \\
    --logger_dir "{log_dir}" \\
    --epochs -1 \\
    --every_plot_step 2000 \\
    --gradient_clip_val 0.5 \\
    --gradient_clip_algorithm "norm" """

# ThÃªm --ckpt_path náº¿u cÃ³ checkpoint
if 'ckpt_path' in locals() and ckpt_path:
    cmd += f'\\\n    --ckpt_path "{ckpt_path}"'

print("ğŸš€ Báº¯t Ä‘áº§u training...")
print("=" * 60)
print(cmd)
print("=" * 60)

# Cháº¡y training
!{cmd}
```

**LÆ°u Ã½:**
- Training sáº½ cháº¡y vÃ  hiá»ƒn thá»‹ log
- CÃ³ thá»ƒ máº¥t vÃ i phÃºt Ä‘á»ƒ khá»Ÿi Ä‘á»™ng
- Checkpoint sáº½ Ä‘Æ°á»£c lÆ°u má»—i 100 steps
- Dataset path sáº½ tá»± Ä‘á»™ng sá»­ dá»¥ng Ä‘Æ°á»ng dáº«n Ä‘Ã£ tÃ¬m tháº¥y á»Ÿ BÆ°á»›c 6

---

## ğŸ“Š BÆ¯á»šC 11: Monitor Training (Optional)

**Táº¡o cell má»›i vÃ  cháº¡y (Ä‘á»ƒ xem log):**

```python
# Xem log real-time (cháº¡y cell nÃ y trong tab má»›i Ä‘á»ƒ khÃ´ng block)
import time

log_dir = "/content/drive/MyDrive/ace_step_outputs/logs/vi_lora/lightning_logs"
log_files = glob.glob(f"{log_dir}/*/events.out.tfevents.*")

if log_files:
    latest_log = max(log_files, key=os.path.getctime)
    print(f"ğŸ“Š Äang theo dÃµi log: {latest_log}")
    print("=" * 60)
    !tail -f {latest_log}
else:
    print("â„¹ ChÆ°a cÃ³ log file")
```

**Hoáº·c xem log Ä‘Æ¡n giáº£n hÆ¡n:**

```python
# Xem 50 dÃ²ng log cuá»‘i cÃ¹ng
log_dir = "/content/drive/MyDrive/ace_step_outputs/logs/vi_lora/lightning_logs"
log_files = glob.glob(f"{log_dir}/*/events.out.tfevents.*")

if log_files:
    latest_log = max(log_files, key=os.path.getctime)
    !tail -n 50 {latest_log}
```

---

## âš ï¸ LÆ¯U Ã QUAN TRá»ŒNG

### 1. Runtime Timeout
- **Colab Free**: ~12 giá» timeout
- **Colab Pro**: ~24 giá» timeout
- **Giáº£i phÃ¡p**: 
  - LÆ°u checkpoint má»—i 100 steps (Ä‘Ã£ set)
  - Resume tá»« checkpoint má»›i nháº¥t khi restart

### 2. Náº¿u Bá»‹ Disconnect
1. TÃ¬m checkpoint má»›i nháº¥t (BÆ°á»›c 9)
2. Resume training (BÆ°á»›c 10) vá»›i checkpoint Ä‘Ã³

### 3. Tá»‘i Æ¯u cho GPU
- **T4 (16GB)**: DÃ¹ng `--accumulate_grad_batches 4` (Ä‘Ã£ set)
- **V100 (16GB)**: CÃ³ thá»ƒ tÄƒng lÃªn `8`
- **A100 (40GB)**: CÃ³ thá»ƒ tÄƒng lÃªn `16`

### 4. Náº¿u Bá»‹ OOM (Out of Memory)
- Giáº£m `--accumulate_grad_batches` xuá»‘ng `2` hoáº·c `1`
- Giáº£m `--num_workers` xuá»‘ng `0`

---

## ğŸ”„ Resume Training (Sau khi Disconnect)

**Náº¿u bá»‹ disconnect, lÃ m láº¡i tá»« BÆ°á»›c 9:**

1. Cháº¡y láº¡i cell BÆ°á»›c 9 (TÃ¬m checkpoint)
2. Cháº¡y láº¡i cell BÆ°á»›c 10 (Training) - sáº½ tá»± Ä‘á»™ng resume

---

## ğŸ“¥ Download Checkpoint vá» Local

**Sau khi training xong, download checkpoint:**

```python
# TÃ¬m checkpoint má»›i nháº¥t
import glob
import os

log_dir = "/content/drive/MyDrive/ace_step_outputs/logs/vi_lora/lightning_logs"
checkpoints = glob.glob(f"{log_dir}/*/checkpoints/*.ckpt")

if checkpoints:
    latest_checkpoint = max(checkpoints, key=os.path.getctime)
    print(f"âœ“ Checkpoint má»›i nháº¥t: {latest_checkpoint}")
    print(f"  ÄÃ£ lÆ°u trÃªn Google Drive, cÃ³ thá»ƒ download vá» local")
else:
    print("â„¹ ChÆ°a cÃ³ checkpoint")
```

**CÃ¡ch download:**
1. Má»Ÿ Google Drive
2. VÃ o folder `ace_step_outputs/logs/vi_lora/lightning_logs/.../checkpoints/`
3. Download file `.ckpt` vá» mÃ¡y

---

## ğŸ¯ Checklist HoÃ n ThÃ nh

- [ ] ÄÃ£ mount Google Drive (BÆ°á»›c 3)
- [ ] ÄÃ£ clone repository (BÆ°á»›c 4)
- [ ] ÄÃ£ cÃ i Ä‘áº·t dependencies (BÆ°á»›c 5)
- [ ] ÄÃ£ kiá»ƒm tra dataset vÃ  config (BÆ°á»›c 6)
- [ ] ÄÃ£ táº¡o thÆ° má»¥c output (BÆ°á»›c 7)
- [ ] ÄÃ£ kiá»ƒm tra GPU (BÆ°á»›c 8)
- [ ] ÄÃ£ tÃ¬m checkpoint (náº¿u resume) (BÆ°á»›c 9)
- [ ] ÄÃ£ báº¯t Ä‘áº§u training (BÆ°á»›c 10)
- [ ] Training Ä‘ang cháº¡y (khÃ´ng cÃ³ lá»—i)

---

## ğŸ†˜ Troubleshooting

### Lá»—i: "Dataset not found"
- Kiá»ƒm tra Google Drive Ä‘Ã£ Ä‘Æ°á»£c mount chÆ°a (BÆ°á»›c 3)
- Äáº£m báº£o Ä‘Ã£ upload folder `vi_lora_dataset` lÃªn Google Drive
- Script sáº½ tá»± Ä‘á»™ng tÃ¬m dataset á»Ÿ báº¥t ká»³ Ä‘Ã¢u trong MyDrive
- Náº¿u váº«n khÃ´ng tÃ¬m tháº¥y, kiá»ƒm tra tÃªn folder cÃ³ Ä‘Ãºng `vi_lora_dataset` khÃ´ng

### Lá»—i: "Config not found"
- Kiá»ƒm tra file `vi_lora_config.json` trÃªn Google Drive
- Script sáº½ tá»± Ä‘á»™ng tÃ¬m vÃ  copy config (BÆ°á»›c 6)
- Náº¿u khÃ´ng tÃ¬m tháº¥y, cÃ³ thá»ƒ sá»­ dá»¥ng config máº·c Ä‘á»‹nh trong repo

### Lá»—i: "Out of Memory"
- Giáº£m `--accumulate_grad_batches` xuá»‘ng `2` hoáº·c `1`
- Giáº£m `--num_workers` xuá»‘ng `0`

### Lá»—i: "Runtime disconnected"
- Resume tá»« checkpoint má»›i nháº¥t (BÆ°á»›c 8-10)
- Äáº£m báº£o cháº¡y láº¡i BÆ°á»›c 6 Ä‘á»ƒ tÃ¬m láº¡i dataset_path

---

## ğŸ“ Ghi ChÃº

- Checkpoint Ä‘Æ°á»£c lÆ°u má»—i **100 steps** (Ä‘Ã£ set `--every_n_train_steps 100`)
- Log Ä‘Æ°á»£c lÆ°u trÃªn Google Drive
- CÃ³ thá»ƒ resume báº¥t cá»© lÃºc nÃ o tá»« checkpoint má»›i nháº¥t
- Training sáº½ tá»± Ä‘á»™ng dá»«ng khi Ä‘áº¡t `max_steps` (20000)

---

**ChÃºc báº¡n training thÃ nh cÃ´ng! ğŸ‰**

