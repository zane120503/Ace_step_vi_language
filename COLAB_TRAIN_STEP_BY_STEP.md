# HÆ°á»›ng dáº«n Train ACE-Step LoRA trÃªn Google Colab - Tá»«ng BÆ°á»›c Chi Tiáº¿t

## ğŸ“‹ Chuáº©n bá»‹ trÆ°á»›c khi báº¯t Ä‘áº§u

### 1. Cáº§n cÃ³:
- âœ… Google Account
- âœ… Google Drive (Ä‘á»ƒ lÆ°u dataset vÃ  checkpoint)
- âœ… Dataset Ä‘Ã£ Ä‘Æ°á»£c convert sang HuggingFace format (`vi_lora_dataset`)
- âœ… File config: `config/vi_lora_config.json`

### 2. Upload lÃªn Google Drive:
- Táº¡o folder `MyDrive/ace_step_data/`
- Upload folder `vi_lora_dataset` vÃ o Ä‘Ã³
- Upload file `config/vi_lora_config.json` vÃ o Ä‘Ã³

---

## ğŸš€ BÆ¯á»šC 1: Má»Ÿ Google Colab

1. Truy cáº­p: https://colab.research.google.com/
2. ÄÄƒng nháº­p báº±ng Google Account
3. Click **"New notebook"** Ä‘á»ƒ táº¡o notebook má»›i
4. Äáº·t tÃªn notebook: `ACE-Step LoRA Training`

---

## ğŸš€ BÆ¯á»šC 2: Chá»n GPU

1. Click **Runtime** â†’ **Change runtime type**
2. Chá»n:
   - **Hardware accelerator**: `GPU`
   - **GPU type**: 
     - **Free**: T4 (tá»± Ä‘á»™ng)
     - **Pro**: T4/V100 (tÃ¹y may máº¯n)
     - **Pro+**: A100 (tÃ¹y may máº¯n)
3. Click **Save**

---

## ğŸš€ BÆ¯á»šC 3: Mount Google Drive

**Táº¡o cell má»›i vÃ  cháº¡y:**

```python
from google.colab import drive
drive.mount('/content/drive')
```

**Káº¿t quáº£:**
- Sáº½ hiá»‡n link Ä‘á»ƒ authorize
- Click link â†’ chá»n Google Account â†’ Copy mÃ£
- Paste mÃ£ vÃ o Ã´ input â†’ Enter
- Sáº½ tháº¥y: `Mounted at /content/drive`

---

## ğŸš€ BÆ¯á»šC 4: Clone Repository

**Táº¡o cell má»›i vÃ  cháº¡y:**

```python
!git clone https://github.com/ace-step/ACE-Step.git
%cd ACE-Step
```

**Káº¿t quáº£:**
- Repository Ä‘Æ°á»£c clone vÃ o `/content/ACE-Step`
- ÄÃ£ chuyá»ƒn vÃ o thÆ° má»¥c ACE-Step

---

## ğŸš€ BÆ¯á»šC 5: CÃ i Ä‘áº·t Dependencies

**Táº¡o cell má»›i vÃ  cháº¡y:**

```python
!pip install -q pytorch-lightning transformers accelerate
!pip install -q -r requirements.txt
```

**LÆ°u Ã½:**
- CÃ³ thá»ƒ máº¥t 5-10 phÃºt
- Náº¿u cÃ³ lá»—i, thá»­ cháº¡y láº¡i cell

---

## ğŸš€ BÆ¯á»šC 6: Kiá»ƒm tra Dataset

**Táº¡o cell má»›i vÃ  cháº¡y:**

```python
import os

# Kiá»ƒm tra dataset cÃ³ tá»“n táº¡i khÃ´ng
dataset_path = "/content/drive/MyDrive/ace_step_data/vi_lora_dataset"
if os.path.exists(dataset_path):
    print(f"âœ“ Dataset tÃ¬m tháº¥y táº¡i: {dataset_path}")
    # Äáº¿m sá»‘ file
    files = os.listdir(dataset_path)
    print(f"âœ“ Sá»‘ file trong dataset: {len(files)}")
else:
    print(f"âŒ KhÃ´ng tÃ¬m tháº¥y dataset táº¡i: {dataset_path}")
    print("   Vui lÃ²ng upload dataset lÃªn Google Drive trÆ°á»›c!")
```

**Káº¿t quáº£ mong Ä‘á»£i:**
- âœ“ Dataset tÃ¬m tháº¥y táº¡i: ...
- âœ“ Sá»‘ file trong dataset: ...

---

## ğŸš€ BÆ¯á»šC 7: Kiá»ƒm tra Config

**Táº¡o cell má»›i vÃ  cháº¡y:**

```python
import os

# Kiá»ƒm tra config file
config_path = "/content/drive/MyDrive/ace_step_data/config/vi_lora_config.json"
if os.path.exists(config_path):
    print(f"âœ“ Config file tÃ¬m tháº¥y táº¡i: {config_path}")
    # Copy vÃ o thÆ° má»¥c config cá»§a repo
    !cp "{config_path}" config/vi_lora_config.json
    print("âœ“ ÄÃ£ copy config vÃ o repo")
else:
    print(f"âŒ KhÃ´ng tÃ¬m tháº¥y config táº¡i: {config_path}")
    print("   Vui lÃ²ng upload config file lÃªn Google Drive trÆ°á»›c!")
```

**Káº¿t quáº£ mong Ä‘á»£i:**
- âœ“ Config file tÃ¬m tháº¥y táº¡i: ...
- âœ“ ÄÃ£ copy config vÃ o repo

---

## ğŸš€ BÆ¯á»šC 8: Táº¡o ThÆ° Má»¥c Output

**Táº¡o cell má»›i vÃ  cháº¡y:**

```python
import os

# Táº¡o thÆ° má»¥c trÃªn Google Drive Ä‘á»ƒ lÆ°u checkpoint vÃ  log
checkpoint_dir = "/content/drive/MyDrive/ace_step_outputs/checkpoints"
log_dir = "/content/drive/MyDrive/ace_step_outputs/logs"

os.makedirs(checkpoint_dir, exist_ok=True)
os.makedirs(log_dir, exist_ok=True)

print(f"âœ“ ÄÃ£ táº¡o thÆ° má»¥c checkpoint: {checkpoint_dir}")
print(f"âœ“ ÄÃ£ táº¡o thÆ° má»¥c log: {log_dir}")
```

**Káº¿t quáº£ mong Ä‘á»£i:**
- âœ“ ÄÃ£ táº¡o thÆ° má»¥c checkpoint: ...
- âœ“ ÄÃ£ táº¡o thÆ° má»¥c log: ...

---

## ğŸš€ BÆ¯á»šC 9: TÃ¬m Checkpoint (Náº¿u Resume)

**Táº¡o cell má»›i vÃ  cháº¡y:**

```python
import glob
import os

# TÃ¬m checkpoint má»›i nháº¥t (náº¿u cÃ³)
log_dir = "/content/drive/MyDrive/ace_step_outputs/logs/vi_lora/lightning_logs"
checkpoints = glob.glob(f"{log_dir}/*/checkpoints/*.ckpt") if os.path.exists(log_dir) else []

if checkpoints:
    latest_checkpoint = max(checkpoints, key=os.path.getctime)
    print(f"âœ“ TÃ¬m tháº¥y checkpoint má»›i nháº¥t: {latest_checkpoint}")
    print(f"  Sáº½ resume tá»« checkpoint nÃ y")
    ckpt_path = latest_checkpoint
else:
    print("â„¹ ChÆ°a cÃ³ checkpoint, sáº½ train tá»« Ä‘áº§u")
    ckpt_path = None
```

**Káº¿t quáº£:**
- Náº¿u cÃ³ checkpoint: âœ“ TÃ¬m tháº¥y checkpoint má»›i nháº¥t: ...
- Náº¿u chÆ°a cÃ³: â„¹ ChÆ°a cÃ³ checkpoint, sáº½ train tá»« Ä‘áº§u

---

## ğŸš€ BÆ¯á»šC 10: Báº¯t Äáº§u Training

**Táº¡o cell má»›i vÃ  cháº¡y:**

```python
import os

# Tham sá»‘ training
dataset_path = "/content/drive/MyDrive/ace_step_data/vi_lora_dataset"
checkpoint_dir = "/content/drive/MyDrive/ace_step_outputs/checkpoints"
log_dir = "/content/drive/MyDrive/ace_step_outputs/logs"

# Build command
cmd = f"""python trainer.py \\
    --num_nodes 1 \\
    --devices 1 \\
    --dataset_path "{dataset_path}" \\
    --exp_name "vi_lora_small" \\
    --lora_config_path "config/vi_lora_config.json" \\
    --learning_rate 1e-4 \\
    --accumulate_grad_batches 4 \\
    --precision 16 \\
    --num_workers 2 \\
    --max_steps 20000 \\
    --every_n_train_steps 100 \\
    --shift 3.0 \\
    --checkpoint_dir "{checkpoint_dir}" \\
    --logger_dir "{log_dir}" \\
    --epochs -1 \\
    --every_plot_step 2000 \\
    --gradient_clip_val 0.5 \\
    --gradient_clip_algorithm "norm" """

# ThÃªm --ckpt_path náº¿u cÃ³ checkpoint
if 'ckpt_path' in locals() and ckpt_path:
    cmd += f'\\\n    --ckpt_path "{ckpt_path}"'

print("ğŸš€ Báº¯t Ä‘áº§u training...")
print("=" * 60)
print(cmd)
print("=" * 60)

# Cháº¡y training
!{cmd}
```

**LÆ°u Ã½:**
- Training sáº½ cháº¡y vÃ  hiá»ƒn thá»‹ log
- CÃ³ thá»ƒ máº¥t vÃ i phÃºt Ä‘á»ƒ khá»Ÿi Ä‘á»™ng
- Checkpoint sáº½ Ä‘Æ°á»£c lÆ°u má»—i 100 steps

---

## ğŸ“Š BÆ¯á»šC 11: Monitor Training

**Táº¡o cell má»›i vÃ  cháº¡y (Ä‘á»ƒ xem log):**

```python
# Xem log real-time (cháº¡y cell nÃ y trong tab má»›i Ä‘á»ƒ khÃ´ng block)
import time

log_dir = "/content/drive/MyDrive/ace_step_outputs/logs/vi_lora/lightning_logs"
log_files = glob.glob(f"{log_dir}/*/events.out.tfevents.*")

if log_files:
    latest_log = max(log_files, key=os.path.getctime)
    print(f"ğŸ“Š Äang theo dÃµi log: {latest_log}")
    print("=" * 60)
    !tail -f {latest_log}
else:
    print("â„¹ ChÆ°a cÃ³ log file")
```

**Hoáº·c xem log Ä‘Æ¡n giáº£n hÆ¡n:**

```python
# Xem 50 dÃ²ng log cuá»‘i cÃ¹ng
log_dir = "/content/drive/MyDrive/ace_step_outputs/logs/vi_lora/lightning_logs"
log_files = glob.glob(f"{log_dir}/*/events.out.tfevents.*")

if log_files:
    latest_log = max(log_files, key=os.path.getctime)
    !tail -n 50 {latest_log}
```

---

## âš ï¸ LÆ¯U Ã QUAN TRá»ŒNG

### 1. Runtime Timeout
- **Colab Free**: ~12 giá» timeout
- **Colab Pro**: ~24 giá» timeout
- **Giáº£i phÃ¡p**: 
  - LÆ°u checkpoint má»—i 100 steps (Ä‘Ã£ set)
  - Resume tá»« checkpoint má»›i nháº¥t khi restart

### 2. Náº¿u Bá»‹ Disconnect
1. TÃ¬m checkpoint má»›i nháº¥t (BÆ°á»›c 9)
2. Resume training (BÆ°á»›c 10) vá»›i checkpoint Ä‘Ã³

### 3. Tá»‘i Æ¯u cho GPU
- **T4 (16GB)**: DÃ¹ng `--accumulate_grad_batches 4` (Ä‘Ã£ set)
- **V100 (16GB)**: CÃ³ thá»ƒ tÄƒng lÃªn `8`
- **A100 (40GB)**: CÃ³ thá»ƒ tÄƒng lÃªn `16`

### 4. Náº¿u Bá»‹ OOM (Out of Memory)
- Giáº£m `--accumulate_grad_batches` xuá»‘ng `2` hoáº·c `1`
- Giáº£m `--num_workers` xuá»‘ng `0`

---

## ğŸ”„ Resume Training (Sau khi Disconnect)

**Náº¿u bá»‹ disconnect, lÃ m láº¡i tá»« BÆ°á»›c 9:**

1. Cháº¡y láº¡i cell BÆ°á»›c 9 (TÃ¬m checkpoint)
2. Cháº¡y láº¡i cell BÆ°á»›c 10 (Training) - sáº½ tá»± Ä‘á»™ng resume

---

## ğŸ“¥ Download Checkpoint vá» Local

**Sau khi training xong, download checkpoint:**

```python
# TÃ¬m checkpoint má»›i nháº¥t
import glob
import os

log_dir = "/content/drive/MyDrive/ace_step_outputs/logs/vi_lora/lightning_logs"
checkpoints = glob.glob(f"{log_dir}/*/checkpoints/*.ckpt")

if checkpoints:
    latest_checkpoint = max(checkpoints, key=os.path.getctime)
    print(f"âœ“ Checkpoint má»›i nháº¥t: {latest_checkpoint}")
    print(f"  ÄÃ£ lÆ°u trÃªn Google Drive, cÃ³ thá»ƒ download vá» local")
else:
    print("â„¹ ChÆ°a cÃ³ checkpoint")
```

**CÃ¡ch download:**
1. Má»Ÿ Google Drive
2. VÃ o folder `ace_step_outputs/logs/vi_lora/lightning_logs/.../checkpoints/`
3. Download file `.ckpt` vá» mÃ¡y

---

## ğŸ¯ Checklist HoÃ n ThÃ nh

- [ ] ÄÃ£ mount Google Drive
- [ ] ÄÃ£ clone repository
- [ ] ÄÃ£ cÃ i Ä‘áº·t dependencies
- [ ] ÄÃ£ kiá»ƒm tra dataset
- [ ] ÄÃ£ kiá»ƒm tra config
- [ ] ÄÃ£ táº¡o thÆ° má»¥c output
- [ ] ÄÃ£ báº¯t Ä‘áº§u training
- [ ] Training Ä‘ang cháº¡y (khÃ´ng cÃ³ lá»—i)

---

## ğŸ†˜ Troubleshooting

### Lá»—i: "Dataset not found"
- Kiá»ƒm tra Ä‘Æ°á»ng dáº«n dataset trÃªn Google Drive
- Äáº£m báº£o Ä‘Ã£ upload Ä‘Ãºng folder `vi_lora_dataset`

### Lá»—i: "Config not found"
- Kiá»ƒm tra file `vi_lora_config.json` trÃªn Google Drive
- Äáº£m báº£o Ä‘Ã£ copy vÃ o repo (BÆ°á»›c 7)

### Lá»—i: "Out of Memory"
- Giáº£m `--accumulate_grad_batches` xuá»‘ng `2` hoáº·c `1`
- Giáº£m `--num_workers` xuá»‘ng `0`

### Lá»—i: "Runtime disconnected"
- Resume tá»« checkpoint má»›i nháº¥t (BÆ°á»›c 9 + 10)

---

## ğŸ“ Ghi ChÃº

- Checkpoint Ä‘Æ°á»£c lÆ°u má»—i **100 steps** (Ä‘Ã£ set `--every_n_train_steps 100`)
- Log Ä‘Æ°á»£c lÆ°u trÃªn Google Drive
- CÃ³ thá»ƒ resume báº¥t cá»© lÃºc nÃ o tá»« checkpoint má»›i nháº¥t
- Training sáº½ tá»± Ä‘á»™ng dá»«ng khi Ä‘áº¡t `max_steps` (20000)

---

**ChÃºc báº¡n training thÃ nh cÃ´ng! ğŸ‰**

