"""
Script Ä‘á»ƒ train ACE-Step LoRA trÃªn Google Colab
Copy vÃ  cháº¡y tá»«ng pháº§n trong Colab notebook
"""

# ============================================
# PHáº¦N 1: Setup mÃ´i trÆ°á»ng (cháº¡y 1 láº§n)
# ============================================

# Mount Google Drive
from google.colab import drive
drive.mount('/content/drive')

# Clone repository (náº¿u chÆ°a cÃ³)
import os
if not os.path.exists('/content/ACE-Step'):
    !git clone https://github.com/ace-step/ACE-Step.git
    %cd /content/ACE-Step
else:
    %cd /content/ACE-Step
    !git pull

# CÃ i Ä‘áº·t dependencies
!pip install -q pytorch-lightning transformers accelerate
!pip install -q -r requirements.txt

# ============================================
# PHáº¦N 2: TÃ¬m checkpoint má»›i nháº¥t (náº¿u resume)
# ============================================

import glob

def find_latest_checkpoint(log_dir="/content/drive/MyDrive/ace_step_outputs/logs/vi_lora/lightning_logs"):
    """TÃ¬m checkpoint má»›i nháº¥t Ä‘á»ƒ resume"""
    checkpoints = glob.glob(f"{log_dir}/*/checkpoints/*.ckpt")
    if checkpoints:
        latest = max(checkpoints, key=os.path.getctime)
        print(f"âœ“ TÃ¬m tháº¥y checkpoint: {latest}")
        return latest
    else:
        print("â„¹ ChÆ°a cÃ³ checkpoint, sáº½ train tá»« Ä‘áº§u")
        return None

# ============================================
# PHáº¦N 3: Train LoRA
# ============================================

def train_lora(
    dataset_path="/content/drive/MyDrive/vi_lora_dataset",
    checkpoint_dir="/content/drive/MyDrive/ace_step_outputs/checkpoints",
    log_dir="/content/drive/MyDrive/ace_step_outputs/logs",
    resume_from_checkpoint=True,
    max_steps=20000,
    every_n_train_steps=500,
    accumulate_grad_batches=4,
    precision=16,
    num_workers=2
):
    """
    Train LoRA trÃªn Colab
    
    Args:
        dataset_path: ÄÆ°á»ng dáº«n Ä‘áº¿n dataset (trÃªn Google Drive)
        checkpoint_dir: ThÆ° má»¥c lÆ°u checkpoint (trÃªn Google Drive)
        log_dir: ThÆ° má»¥c lÆ°u log (trÃªn Google Drive)
        resume_from_checkpoint: CÃ³ tá»± Ä‘á»™ng resume tá»« checkpoint má»›i nháº¥t khÃ´ng
        max_steps: Sá»‘ step tá»‘i Ä‘a
        every_n_train_steps: LÆ°u checkpoint má»—i N steps
        accumulate_grad_batches: Gradient accumulation
        precision: 16 (FP16) hoáº·c 32 (FP32)
        num_workers: Sá»‘ worker cho DataLoader (Colab nÃªn dÃ¹ng 2)
    """
    
    # Táº¡o thÆ° má»¥c output
    os.makedirs(checkpoint_dir, exist_ok=True)
    os.makedirs(log_dir, exist_ok=True)
    
    # TÃ¬m checkpoint Ä‘á»ƒ resume
    ckpt_path = None
    if resume_from_checkpoint:
        ckpt_path = find_latest_checkpoint(log_dir)
    
    # Build command
    cmd = f"""python trainer.py \\
    --num_nodes 1 \\
    --devices 1 \\
    --dataset_path "{dataset_path}" \\
    --exp_name "vi_lora_small" \\
    --lora_config_path "config/vi_lora_config.json" \\
    --learning_rate 1e-4 \\
    --accumulate_grad_batches {accumulate_grad_batches} \\
    --precision {precision} \\
    --num_workers {num_workers} \\
    --max_steps {max_steps} \\
    --every_n_train_steps {every_n_train_steps} \\
    --shift 3.0 \\
    --checkpoint_dir "{checkpoint_dir}" \\
    --logger_dir "{log_dir}" \\
    --epochs -1 \\
    --every_plot_step 2000 \\
    --gradient_clip_val 0.5 \\
    --gradient_clip_algorithm "norm" """
    
    if ckpt_path:
        cmd += f'\\\n    --ckpt_path "{ckpt_path}"'
    
    print("ğŸš€ Báº¯t Ä‘áº§u training...")
    print(f"Command: {cmd}")
    
    # Cháº¡y training
    !{cmd}

# ============================================
# PHáº¦N 4: Sá»­ dá»¥ng
# ============================================

# CÃ¡ch 1: Train tá»« Ä‘áº§u
# train_lora(
#     dataset_path="/content/drive/MyDrive/vi_lora_dataset",
#     resume_from_checkpoint=False
# )

# CÃ¡ch 2: Resume tá»« checkpoint má»›i nháº¥t
# train_lora(
#     dataset_path="/content/drive/MyDrive/vi_lora_dataset",
#     resume_from_checkpoint=True
# )

# CÃ¡ch 3: Train vá»›i tham sá»‘ tÃ¹y chá»‰nh
# train_lora(
#     dataset_path="/content/drive/MyDrive/vi_lora_dataset",
#     max_steps=50000,
#     every_n_train_steps=200,  # LÆ°u checkpoint thÆ°á»ng xuyÃªn hÆ¡n
#     accumulate_grad_batches=8,  # TÄƒng náº¿u GPU Ä‘á»§ máº¡nh
#     num_workers=0  # Giáº£m náº¿u bá»‹ lá»—i
# )

